* Initial concerns:
  * finalize architecture
  * how to pull training data? especially when personal computers have limited capacities, usage of VM's? Gaining access to production workloads (due to privacy concerns), TPC-C, TPC-B is currently lacking.
  * Disk speed is often the most important factor in a DBMS’s performance. Although the previous studies used virtualized environments to evaluate their methods, to our knowledge, they deploy the DBMS on ephemeral storage that is physically attached to the host machine. But many real-world DBMS deployments use durable, non-local storage for data and logs, such as on-premise SANs and cloud-based block/object stores. The problem with these non-local storage devices is that their performance can vary substantially in a multi-tenant cloud environment



## **Ottertune**

Other tuning tools:

1.  https://pgtune.leopard.in.ua/#/
2. http://www.vldb.org/pvldb/vol2/vldb09-193.pdf (ITUNED)
3. Reinforcement learning (https://link.springer.com/chapter/10.1007/978-3-030-30639-7_9)
   1. https://www.vldb.org/pvldb/vol12/p2118-li.pdf (QTUNE)
   2. https://dbgroup.cs.tsinghua.edu.cn/ligl/papers/sigmod19-cdbtune.pdf (CBDTUNE) > use of DDBG
4.  biggest spike given two knobs in Postgres: SHARED_BUFFERS, MAX_WAL_SIZE

* Receives $2.5 million in seed funding, led by Accel, now in private beta 

1. **[Automatic Database Management System Tuning through ML (2017)](http://www.cs.cmu.edu/~ggordon/van-aken-etal-parameters.pdf)**

* knobs are not universal between databases, they are also not independent (changing one knob can affect another), not universal (what works for one application may be sub-optimal for another)

* automated approach:

  * we use a combination of supervised and unsupervised machine learning methods to:

     (1) select the most impactful knobs (2) map unseen database workloads to previous workloads from which we can transfer experience (3) then recommend new knob settings

  * 3 DBMS tested (OLTP DBMSs (MySQL, Postgres) and one OLAP DBMS (Vector).
    * performance and scalability are highly dependent on their configurations of knobs; default configs tend to be bad
    * Postgres has around 300 knobs in 2016, mysql has 600 knobs in 2016

* performance measured by:

  * throughput (how fast it can collect new data)
  * latency (how fast it can respond to a request)

* how is this different from previous attempts? 

  * created by vendors that supported an only particular company's DBMS workload and settings
  * those who support multiple DBMS require manual steps still or intervention by DBA
  * examines each DBMS deployment independently, not using knowledge gained from previous tuning

  

  **Overall summary:**

  The crux of our approach to reuse data from previous sessions is to train machine learning (ML) models from measurements collected from these previous tunings, and use the models to (1) select the most important knobs, (2) map previously unseen database workloads to known workloads, so that we can transfer previous experience, and (3) recommend knob settings that improve a target objective (e.g., latency, throughput).

  * Reusing past experience reduces the amount of time and resources it takes to tune a DBMS for a new application. To evaluate our work, we implemented our techniques using Google TensorFlow and Python’s scikit-learn

**Results: **

* 58–94% lower latency compared to their default settings or configurations generated by other tuning advisors. OtterTune generates configurations in under 60 min that are within 94% of ones created by expert DBAs.



**Motivation:**

* "best practice guidelines" do not generalize to all applications/hardware configurations
* One common approach to tuning a DBMS is "trial and error": for the DBA to copy the database to another machine and manually measure the performance of a sample workload from the real application. Then tweak configurations according to some tuning guidelines (and intuition based on past experiences)
  * bad since (1) many of the knobs are not independent, (2) the values for some knobs are continuous, (3) one often cannot reuse the same configuration from one application to the next, and (4) DBMSs are always adding new knobs
  * it is difficult enough for humans to understand the impact of one knob let alone the interactions between multiple ones. The different combinations of knob settings means that finding the optimal configuration is NP-hard
  * shows examples of problems with knob: dependencies, continuous settings, non-reusable configurations, and tuning complexities in Motivation

**Architecture:**

![image-20210614160029638](C:\Users\ferdi\AppData\Roaming\Typora\typora-user-images\image-20210614160029638.png)

* The system is comprised of two parts.
  * 1) The first is the client-side controller that interacts with the target DBMS to be tuned. It collects runtime information from the DBMS using a standard API (e.g., JDBC), installs new configurations, and collects performance measurements.
    * They use JDBC ( Java Database Connectivity API) to retrieve information from the database
  * 2)  Second is OtterTune’s tuning manager. It receives the information collected from the controller and stores it in its repository with data from previous tuning sessions. 
    * This repository does not contain any confidential information about the DBMSs or their databases; it only contains knob configurations and performance data.
    *  OtterTune organizes this data per major DBMS version (e.g., Postgres v9.3 data is separate from Postgres v9.4). This prevents OtterTune from tuning knobs from older versions of the DBMS that may be deprecated in newer versions, or tuning knobs that only exist in newer versions. 
    * The manager is also supported by background processes that continuously analyze new data and refine OtterTune’s internal ML models. These models allow it to identify the relevant knobs and metrics without human input, and find workloads in the repository that are similar to the target.

**Example**

1. DBA chooses which to optimize (latency or throughput), controller connects to DBMS and collects hardwire profile and current knob configuration

2. Controller does first observation period (measure chosen metric), and other DBMS-specific internal metrics (Examples include MySQL’s counters for pages read to or written from disk.)

   * 1. Execute either a set of queries for a fixed time period;  then the length of the observation period is equal to the fixed time period. 
   * 2. Execute a specific workload trace ( all scheduling relevant aspects of batch jobs). Then, the duration depends on how long it takes for the DBMS to replay the workload trace.
   *  Fixed observation periods are well-suited for the fast, simple queries that are characteristic of OLTP workloads, whereas variable-length periods are often necessary for executing the longrunning, complex queries present in OLAP workloads. 
   * The controller first resets all of the statistics for the target DBMS. It then retrieves the new statistics data at the end of the period. Since at this point OtterTune does not know which metrics are actually useful, it collects every numeric metric that the DBMS makes available and stores it as a key/value pair in its repository
     * Some systems, like MySQL, only report aggregate statistics for the entire DBMS. Other systems, however, provide separate statistics for tables or databases
     * One potential solution is to prefix the name of the sub-element to the metric’s name. For example, Postgres’ metric for the number of blocks read for the table “foo” would be stored in the repository as foo.heap_blks_read. But this approach means that it is unable to map this metric to other databases since they will have different names for their tables.
       * OtterTune instead stores the metrics with the same name as a single sum scalar value. This works because OtterTune currently only considers global knobs. Tuning table- or component-specific knobs is for future updates
   * Then it prunes, considering the smallest set of metrics that capture the variability in performance and distinguishing characteristics for different workloads; Reducing the size of this set reduces the search space of ML algorithms, which in turn speeds up the entire process and increases the likelihood that the models will fit in memory on OtterTune’s tuning manager'
     * some metrics are redudant sicne different graunularity (in bytes vs pages); or ones that represent independent components of the DBMS but whose values are strongly correlated (Postgres: number of tuples updated moves almost in unison with the metric that measures the number of blocks read from the buffer for indexes)
     * Uses PCA or factor analysis > then apply k-m,means to cluster data into meaningful groups to find redundancies
       * In Postgres the metrics are clustered on specific components in the system, like the background writer and indexes; Postgres metrics (57 to 11 metrics used)
       * Although pruned, still kept in repository

3. Tuning manager receives results of observation period from controller, storing that information in repository, then maps a workload based on the statistics gathered

4. Tuning manager computes next configuration that she be installed on target DBMS based on results collected

   1. First, OtterTune tries to “understand” the target workload and map it to a workload for the same DBMS and hardware profile that it has seen (and tuned) in the past. Once the tuning manager has found the best match using the data that it has collected so far, it then starts the second step where it recommends a knob configuration that is specifically designed to improve the target objective for the current workload, DBMS, and hardware
      * OtterTune uses Gaussian Process (GP) regression and uses gradient descent to find the local optimum on the surface predicted by the GP model using a set of configurations, called the initialization set, as starting points.
   2. OtterTune then provides the controller with an estimate of how much better the recommended configuration is compared to the best configuration that it has seen so far; up to DBA whether or not to keep tuning or not

   

   **Other facts**

   1. Ottertune maintains a curated black-list of knobs for each DBMS version that is supported by OtterTune
      * The DBA is provided with this black-list of knobs at the start of each tuning session. The DBA is permitted to add to this list any other knobs that they want OtterTune to avoid tuning. Such knobs could either be ones that do not make sense to tune (e.g., path names of where the DBMS stores files) or ones that could have hidden or serious consequences (e.g., potentially causing the DBMS to lose data)
   2. Assumes that the physical design of the database is reasonable. That means that the DBA has already installed the proper indexes, materialized views, and other database elements.
   3. Restarting the DBMS is often necessary because some knobs only take effect after the system is stopped and started.
      * OtterTune currently ignores the cost of restarting the DBMS in its recommendations; for example, restarting causes the DBMS to perform extra processing when it comes back on-line (e.g., resizing log files)
      * many DBMSs support changing some knobs dynamically without having to restart the system; OtterTune stores a list of the dynamic knobs that are available on each of the DBMS versions that it supports, as well as the instructions on how to update them. It then restarts the DBMS only when the set of knobs being tuned requires it. The DBA can also elect to tune only dynamic knobs at the start of the tuning session
   4.  OtterTune characterizes a workload using the runtime statistics recorded while executing it. These metrics provide a more accurate representation of a workload because they capture more aspects of its runtime behavior. Some examples from InnoDB (statistics on the number of pages read/written, query cache utilization, and locking overhead)
      * this step is detailed by 6.1 in the paper
   5. Ottertune use an incremental approach where OtterTune dynamically increases the number of knobs used in a tuning session over time (after feature selection of finding which is the best knobs to change)

**ML Pipeline:**

![image-20210614165252849](C:\Users\ferdi\AppData\Roaming\Typora\typora-user-images\image-20210614165252849.png)

* OtterTune uses a popular feature selection technique for linear regression, called Lasso, to expose the knobs that have the strongest correlation to the system’s overall performance (The independent variables are the DBMS’s knobs (or functions of these knobs) and the dependent variables are the metrics that OtterTune collects during an observation period from the DBMS)
  * In order to detect nonlinear correlations and dependencies between knobs, we also include polynomial features (of the knobs by doing interactions with each knob, or product with one another) in our regression.
  * Figure out some feature selections to decide which knobs have the biggest impact on performance; uses Lasso Path algorithm to rank knobs in an ordered list of importance
  * OtterTune’s tuning manager performs these computations continuously in the background as new data arrives from different tuning sessions. In our experiments, each invocation of Lasso takes ∼20 min and consumes ∼10 GB of memory for a repository comprised of 100k trials with millions of data points. The dependencies and correlations that we discover are then used in OtterTune’s recommendation algorithms



2. **[A Demonstration of the Ottertune Automatic DBMS Tuning Service](http://www.vldb.org/pvldb/vol11/p1910-zhang.pdf)** (2018)

* **Updated Architecture:** 

  * information from controller stored in .json files that is sent to tuning manager in django (this info sent to both data repository and job schedulers and celery)
  * The tuning manager is written in Python using the Django web framework. We use the same MySQL v5.7 database for OtterTune’s data repository and the Django back-end database
    * use Django’s ORM APIs to design and create all of the tables
  * We use Celery to schedule and execute tasks for creating OtterTune’s ML models and recommending new configurations. Celery is a task queue and scheduler that is easy to integrate with web frameworks like Django
  * same ML framework through scikit and tensorflow

  ![image-20210615141804019](/Users/jay/Library/Application Support/typora-user-images/image-20210615141804019.png)

  * The controller (1) sends the information to the tuning manager, (2) gets a token from the tuning manager, (3) uses this token to check status of the tuning job, and (4) gets the recommended configuration when the job finishes
  * As of March 2018, OtterTune supports Postgres (v9), MySQL (v5), MyRocks, VectorWise, and Greenplum
  * Postgres v9.4 includes a metric table about its WAL archiver process’s activity2 , but this table is not available in earlier version
  * This research was funded (in part) by the U.S. National Science Foundation (III-1423210), the National Science Foundation’s Graduate Research Fellowship Program (DGE-1252522), and AWS Cloud Credits for Research. (stacked funding)

3. [**An Inquiry into ML-based automatic config tuning services on real-world DBMS **](http://vldb.org/pvldb/vol14/p1241-aken.pdf)(2021)

* seeks to solve if using ML approaches is actually worth it, especially when previous tests are not mimicking actual DBMS in production
  * the data and workload trace that we use in our study came from an internal issue tracking application (TicketTracker) for SG’s IT infrastructure. The core functionality of TicketTracker is similar to other widely used project management software, such as Atlassian Jira and Mozilla Bugzilla. This application keeps track of work tickets submitted across the entire organization. SG has ∼140,000 employees spread across the globe [6], and thus TicketTracker’s workload patterns and query arrival rate are mostly uniform 24- hours a day during the work week. SG currently runs TicketTracker on Oracle v12.1
  * There are important differences in the TicketTracker application compared to the TPC-C benchmark used in previous ML tuning evaluations. Foremost is that the TicketTracker database has hundreds of tables and the TPC-C database only has nine. TPC-C also has a much higher write ratio for queries (46%) than the TicketTracker workload (10%). This finding is consistent with previous work that has compared TPC-C with real-world workloads
  * after looking at results, so much variability in actual dbms that make it almost hard to argue to use a certain ML algorithm since a cloud environment is prone to noise
* moved from GPR (gausian regression) into DNN
* contains comparisons of modern tuning algorithms and what conditions cause which ones to perform better than others; Ottertune then includes all algorithms in tuning manager

![image-20210615150842987](/Users/jay/Library/Application Support/typora-user-images/image-20210615150842987.png)

> that performance boost from other algorithms isn't that required, just doing two knobs is good enough for performance boost in throughput



4. Ottertune Article (https://www.prnewswire.com/news-releases/ottertune-launches-to-improve-database-performance-through-automated-database-configuration-tuning-301289311.html)

5. AWS Article (https://aws.amazon.com/blogs/machine-learning/tuning-your-dbms-automatically-with-machine-learning/)



## General Tuning

* for more information on variables: https://wiki.postgresql.org/wiki/Tuning_Your_PostgreSQL_Server
  * other server config variables: https://hub.packtpub.com/server-configuration-tuning-postgresql/
  * downloaded some files from 2008, https://www.pgcon.org/2008/schedule/events/104.en.html
* General recommendations to improve throughput/latency based on certain workloads (these will be used to confirm that the recommendations from the ML model are accurate):
  1. http://www.vm.ibm.com/education/lvc/LVC0228.pdf (also outlined from: https://developer.ibm.com/technologies/systems/tutorials/postgresql-experiences-tuning-recomendations-linux-on-ibm-z/)
     1. https://linux.mainframe.blog/postgresql-tuning-linux-on-z/

### When they take effect

PostgreSQL settings have different levels of flexibility for when they can be changed, usually related to internal code restrictions. The complete list of levels is:

- Postmaster: requires restart of server
- Sighup: requires a HUP of the server, either by kill -HUP (usually -1), pg_ctl reload, or `SELECT pg_reload_conf()`;
- User: can be set within individual sessions, take effect only within that session
- Internal: set at compile time, can't be changed, mainly for reference
- Backend: settings which must be set before session start
- Superuser: can be set at runtime for the server by superusers

>  Most of the time you'll only use the first of these, but the second can be useful if you have a server you don't want to take down, while the user session settings can be helpful for some special situations. You can tell which type of parameter a setting is by looking at the "context" field in the` pg_settings` view. 

### Important notes about configuration files

- Command line options override postgresql.auto.conf settings override postgresql.conf settings.
- If the same setting is listed multiple times, the last one wins.
- You can figure out the postgresql.conf location with `SHOW config_file`. It will generally be $PGDATA/postgresql.conf (`SHOW data_directory`), but watch out for symbolic links, [postmaster.opts](http://www.postgresql.org/docs/current/static/app-pg-ctl.html#AEN93617) and other trickiness
- Lines with # are comments and have no effect. For a new database, this will mean the setting is using the default, but on running systems this may not hold true! Changes to the configuration files do not take effect without a reload/restart, so it's possible for the system to be running something different from what is in the file.

### Viewing the current settings

- Look at the configuration files. This is generally not definitive!
- `SHOW ALL`, `SHOW <setting>` will show you the current value of the setting. Watch out for session specific changes
- `SELECT * FROM pg_settings` will label session specific changes as locally modified

**Tips**

- [dbForge Studio for PostgreSQL](https://www.devart.com/dbforge/postgresql/studio/query-profiler.html) helps to identify productivity bottlenecks, and provides PostgreSQL performance tuning.
- The [postgresqltuner.pl](https://github.com/jfcoz/postgresqltuner) script can analyze the configuration and make tuning recommendations.
- [PgBadger](https://pgbadger.darold.net/) analyse PostgreSQL logs to generate performance reports.
- [pgMustard](https://www.pgmustard.com/) provides tuning advice based on EXPLAIN ANALYZE output.



**DONT USE PG BENCH**

> TPC-B is not an OLTP benchmark. Rather, TPC-B can be looked at as a database stress test, characterized by:
>
> - Significant disk input/output
>- Moderate system and application execution time
> - Transaction integrity
> 
> TPC-B measures throughput in terms of how many transactions per second a system can perform. Because there are substantial differences between the two benchmarks (OLTP vs. database stress test), TPC-B results cannot be compared to TPC-A.

> ALSO:
>
> The most complex benchmark used to evaluate ML-based tuning techniques to date is the TPC-C OLTP benchmark from the early 1990s. But previous studies have found that some characteristics of TPC-C are not representative of real-world database applications 
>
> Many of the unrealistic aspects of TPC-C are due to its simplistic database schema and query complexity. Another notable difference is the existence of temporary and large objects in production databases. Some DBMSs provide knobs for tuning these objects (e.g., Postgres, Oracle), which have not been considered in prior work.